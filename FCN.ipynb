{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FCN.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"BJ8F3S8oxstu","colab_type":"code","colab":{}},"source":["from tensorflow.python.keras.models import Model\n","from tensorflow.python.keras.utils import np_utils\n","from tensorflow.python.keras.layers import Dropout\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from tensorflow.python import keras\n","from tensorflow.python.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, TensorBoard, EarlyStopping\n","import pandas as pd\n","import time\n","from sklearn import preprocessing\n","from sklearn.model_selection import KFold\n","import os\n","import tensorflow as tf\n","from keras import regularizers\n","\n","tf.compat.v1.disable_eager_execution()\n","from tensorflow.compat.v1 import ConfigProto, Session\n","from tensorflow.compat.v1 import InteractiveSession\n","\n","drive_path = \"Stead_Classification/\"\n","outPath = drive_path +\"FCN/Normalizers_Sample\"\n","\n","train_data = np.load(drive_path +'train-BH-7500*2.npy')\n","test_data = np.load(drive_path +'test-BH-374*2.npy')\n","\n","# Shuffle data\n","np.random.shuffle(train_data)\n","np.random.shuffle(test_data)\n","\n","x_data = train_data[:,1:]\n","y_data = train_data[:,0]\n","x_test = test_data[:,1:]\n","y_test = test_data[:,0]\n","\n","# Sample Standardization\n","x_data = [(x-np.mean(x))/np.std(x) for x in x_data]\n","x_test = [(x-np.mean(x))/np.std(x) for x in x_test]\n","\n","# Reshape data\n","x_data = np.array(x_data)\n","x_test = np.array(x_test)\n","x_data = x_data.reshape(x_data.shape + (1,))\n","x_test = x_test.reshape(x_test.shape + (1,))\n","\n","nb_epochs = 100\n","kf_val = 5\n","batch_list = [1,2,4,8,16,32,64,128,256,512]\n","\n","for batch_size in batch_list:\n","    test = []\n","    train = []\n","    val = []\n","    print(\"###################### Batch = \" +str(batch_size) +\" ######################\")\n","\n","    ################ Cross-Val #####################\n","    kf = KFold(n_splits=kf_val, shuffle=False)\n","    itr = 0\n","    for train_idx, val_idx in kf.split(x_data):\n","        start_time = time.time()\n","        f = open(outPath +\"/stat.txt\", \"a+\")\n","        itr += 1\n","        print(\"********* Iteration \" +str(itr) +\" *********\")\n","\n","        x_train, y_train = x_data[train_idx], y_data[train_idx]\n","        x_val, y_val = x_data[val_idx], y_data[val_idx]\n","        nb_classes = len(np.unique(y_val))\n","        Y_train = np_utils.to_categorical(y_train, nb_classes)\n","        Y_val = np_utils.to_categorical(y_val, nb_classes)\n","        Y_test = np_utils.to_categorical(y_test, nb_classes)\n","        \n","        x = keras.layers.Input(x_train.shape[1:])\n","        conv1 = keras.layers.Conv1D(64, 5, padding='same', kernel_initializer=keras.initializers.glorot_uniform())(x)\n","        conv1 = keras.layers.normalization.BatchNormalization()(conv1)\n","        conv1 = keras.layers.Activation('relu')(conv1)\n","\n","        conv2 = keras.layers.Conv1D(128, 5, padding='same', kernel_initializer=keras.initializers.glorot_uniform())(conv1)\n","        conv2 = keras.layers.normalization.BatchNormalization()(conv2)\n","        conv2 = keras.layers.Activation('relu')(conv2)\n","\n","        conv3 = keras.layers.Conv1D(128, 3, padding='same', kernel_initializer=keras.initializers.glorot_uniform())(conv2)\n","        conv3 = keras.layers.normalization.BatchNormalization()(conv3)\n","        conv3 = keras.layers.Activation('relu')(conv3)\n","\n","        conv4 = keras.layers.Conv1D(64, 3, padding='same', kernel_initializer=keras.initializers.glorot_uniform())(conv3)\n","        conv4 = keras.layers.normalization.BatchNormalization()(conv4)\n","        conv4 = keras.layers.Activation('relu')(conv4)\n","\n","        full = keras.layers.pooling.GlobalAveragePooling1D()(conv4)\n","        out = keras.layers.Dense(nb_classes, activation='softmax', activity_regularizer=regularizers.l2(1e-5))(full)\n","        model = Model(inputs=x, outputs=out)\n","\n","        optimizer = keras.optimizers.Adam()\n","        model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n","\n","        path_checkpoint = outPath +'/checkpoint-batch=' +str(batch_size) +'-' +str(itr) +'.keras'\n","        callback_checkpoint = ModelCheckpoint(filepath=path_checkpoint,\n","                                        monitor='val_loss',\n","                                        verbose=0,\n","                                        save_weights_only=True,\n","                                        save_best_only=True)\n","        callback_reduce_lr = ReduceLROnPlateau(monitor='val_loss',factor=0.1,min_lr=0,patience=10,verbose=0)\n","        callback_tensorboard = TensorBoard(log_dir= outPath +'/log-batch=' +str(batch_size) +'-' +str(itr),\n","                                histogram_freq=0,\n","                                write_graph=False,\n","                                write_images=False)\n","        callback_early_stopping = EarlyStopping(monitor='val_loss', patience=30, verbose=1)\n","\n","        hist = model.fit(x_train, Y_train, batch_size=batch_size, epochs=nb_epochs,\n","                verbose=1, validation_data=(x_val, Y_val),\n","                callbacks=[callback_checkpoint, callback_early_stopping, callback_reduce_lr, callback_tensorboard])\n","        log = pd.DataFrame(hist.history)\n","\n","    ################### TEST DATA\n","        tr = 0\n","        fl = 0\n","        tp = 0\n","        tn = 0\n","        fp = 0\n","        fn = 0\n","        try:\n","            model.load_weights(path_checkpoint)\n","        except Exception as error:\n","            print(\"Error trying to load checkpoint.\")\n","            print(error)\n","\n","        y_predicted = model.predict(x_test)\n","\n","        for i in range(len(Y_test)):\n","            if  np.argmax(Y_test[i]) == np.argmax(y_predicted[i]):\n","                tr += 1\n","                if np.argmax(Y_test[i]) == 0:\n","                    tn += 1\n","                elif np.argmax(Y_test[i]) == 1:\n","                    tp += 1\n","\n","            else:\n","                fl += 1\n","                if np.argmax(Y_test[i]) == 0:\n","                    fn += 1\n","                elif np.argmax(Y_test[i]) == 1:\n","                    fp += 1\n","        test.append(float(tr)/(tr+fl))\n","        np.save(outPath +'/loss-'+str(batch_size)+'-'+str(itr), hist.history['loss'])\n","        np.save(outPath +'/val-loss-'+str(batch_size)+'-'+str(itr), hist.history['val_loss'])\n","        np.save(outPath +'/acc-'+str(batch_size)+'-'+str(itr), hist.history['accuracy'])\n","        np.save(outPath +'/val-acc-'+str(batch_size)+'-'+str(itr), hist.history['val_accuracy'])\n","        plt.plot(hist.history['loss'], label=\"Train\")\n","        plt.plot(hist.history['val_loss'], label=\"Validation\")\n","        plt.title('Model loss')\n","        plt.ylabel('Loss')\n","        plt.xlabel('Epoch')\n","        plt.legend()\n","        plt.savefig(outPath +'/Loss-batch='+str(batch_size)+'-'+str(itr) +'.png', dpi=600)\n","        plt.close()\n","        plt.plot(hist.history['accuracy'], label=\"Train\")\n","        plt.plot(hist.history['val_accuracy'], label=\"Validation\")\n","        plt.title('Model accuracy')\n","        plt.ylabel('Accuracy')\n","        plt.xlabel('Epoch')\n","        plt.legend()\n","        plt.savefig(outPath +'/Accuracy-batch='+str(batch_size)+'-'+str(itr) +'.png', dpi=600)\n","        plt.close()\n","        \n","        train.append(log.loc[log['loss'].idxmin]['accuracy'])\n","        val.append(log.loc[log['loss'].idxmin]['val_accuracy'])\n","        f.write(\"train: \" +str(log.loc[log['loss'].idxmin]['accuracy']) +\"\\tval: \" +str(log.loc[log['loss'].idxmin]['val_accuracy']) +\"\\ttest: \" +str(float(tr)/(tr+fl)) +\"\\n\")\n","        f.write(\"tp: \" +str(tp) +\"\\tfp: \" +str(fp) +\"\\ttn: \" +str(tn) +\"\\tfn: \" +str(fn) +\"\\n\")\n","        f.write(\"--- \" +str(time.time() - start_time) +\" seconds ---\\n\")\n","        print(\"\\n--- \" +str(time.time() - start_time) +\" seconds ---\\n\")\n","        f.close()\n","        \n","    f = open(outPath +\"/stat.txt\", \"a+\")\n","    f.write(\"FOR BATCH = \" +str(batch_size) +\" (train/val/test) ==> \" +str(np.asarray(train).mean()) +\"/\" +str(np.asarray(val).mean()) +\"/\" +str(np.asarray(test).mean()) +\"\\n\")\n","    f.write(\"std train = \" +str(np.std(np.asarray(train))) +\"\\tstd val = \" +str(np.std(np.asarray(val))) +\"\\tstd test = \" +str(np.std(np.asarray(test))) +\"\\n\")\n","    f.close()\n","    plt.plot(train, label=\"train\")\n","    plt.plot(val, label=\"val\")\n","    plt.plot(test, label=\"test\")\n","    plt.legend()\n","    # plt.show()\n","    plt.title(\"batch=\" +str(batch_size))\n","    plt.savefig(outPath +\"/batch=\" +str(batch_size) +'.png', dpi=600)\n","    plt.close()\n"],"execution_count":0,"outputs":[]}]}